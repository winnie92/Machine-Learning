{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "#-------------------------------------------------------------------------\n",
    "'''\n",
    "    Problem 2: Logistic Regression \n",
    "    In this problem, you will implement the logistic regression method for binary classification problems.\n",
    "    The main goal of this problem is to get familiar with a model-based classification method, and how to train the model parameters on the training data.\n",
    "    We will get familiar with gradient computation using the chain rule. \n",
    "    We will use cross entropy as the loss function and stochastic gradient descent to train the model parameters.\n",
    "    You could test the correctness of your code by typing `nosetests test2.py` in the terminal.\n",
    "\n",
    "    Notations:\n",
    "            ---------- input data ----------------------\n",
    "            p: the number of input features.\n",
    "            x: the feature vector of a data instance, a float numpy matrix of shape p by 1. \n",
    "            y: the label of a training instance, an integer scalar value. The values can be 0 or 1.\n",
    "\n",
    "            ---------- model parameters ----------------------\n",
    "            w: the weights parameter of the logistic model, a float numpy matrix of shape p by 1. \n",
    "            b: the bias parameter of the logistic model, a float scalar.\n",
    "\n",
    "            ---------- values ----------------------\n",
    "            z: the logit value, a float scalar\n",
    "            a: the activation value, a float scalar\n",
    "            L: the cross entropy loss value, a float scalar.\n",
    "\n",
    "            ---------- partial gradients ----------------------\n",
    "            dL_da: the partial gradient of the loss function L w.r.t. the activation a, a float scalar value. It represents (d_L / d_a)\n",
    "            da_dz: the partial gradient of the activation a w.r.t. the logit z, a float scalar value. It represents (d_a / d_z)\n",
    "            dz_dw: the partial gradients of the logit z w.r.t. the weights w, a numpy float matrix of shape (p by 1). It represents (d_z / d_w)\n",
    "                   The i-th element represents ( d_z / d_w[i])\n",
    "            dz_db: the partial gradient of logit z w.r.t. the bias b, a float scalar. It represents (d_z / d_b).\n",
    "\n",
    "            ---------- partial gradients of parameters ------------------\n",
    "            dL_dw: the partial gradient of the loss function L w.r.t. the weight vector w, a numpy float matrix of shape (p by 1). \n",
    "                   The i-th element represents ( d_L / d_w[i])\n",
    "            dL_db: the partial gradient of the loss function L w.r.t. the bias b, a float scalar. \n",
    "\n",
    "            ---------- training ----------------------\n",
    "            alpha: the step-size parameter of gradient descent, a float scalar.\n",
    "            n_epoch: the number of passes to go through the training dataset in the training process, an integer scalar.\n",
    "'''\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# Forward Pass \n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "#--------------------------\n",
    "def compute_z(x,w,b):\n",
    "    '''\n",
    "        Compute the linear logit value of a data instance. z = <w, x> + b\n",
    "        Here <w, x> represents the dot product of the two vectors.\n",
    "        Input:\n",
    "            x: the feature vector of a data instance, a float numpy matrix of shape p by 1. \n",
    "            w: the weights parameter of the logistic model, a float numpy matrix of shape p by 1. \n",
    "            b: the bias value of the logistic model, a float scalar.\n",
    "        Output:\n",
    "            z: the logit value of the instance, a float scalar\n",
    "        Hint: you could solve this problem using 1 line of code. Though using more lines of code is also okay.\n",
    "    '''\n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "\n",
    "    z = x.T.dot(w) + b\n",
    "    #########################################\n",
    "    return z \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------------------------\n",
    "def compute_a(z):\n",
    "    '''\n",
    "        Compute the sigmoid activation.\n",
    "        Input:\n",
    "            z: the logit value of logistic regression, a float scalar.\n",
    "        Output:\n",
    "            a: the activation, a float scalar\n",
    "    '''\n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "    if z>700:\n",
    "        a = float(1 / (1 + np.exp(-700)))\n",
    "    elif z<-700:\n",
    "        a =float(1 / (1 + np.exp(700)))\n",
    "    else:\n",
    "        a = float(1 / (1 + np.exp(-z)))\n",
    "    \n",
    "\n",
    "    #########################################\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#--------------------------\n",
    "def compute_L(a,y):\n",
    "    '''\n",
    "        Compute the loss function: the negative log likelihood, which is the negative logarithm of the likelihood. \n",
    "        This function is also called cross-entropy.\n",
    "        Input:\n",
    "            a: the activation of a training instance, a float scalar\n",
    "            y: the label of a training instance, an integer scalar value. The values can be 0 or 1.\n",
    "        Output:\n",
    "            L: the loss value of logistic regression, a float scalar.\n",
    "    '''\n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "    if y == 1:\n",
    "        if a == 0:\n",
    "            L = np.exp(700)\n",
    "        else:\n",
    "            L = -np.log(a)\n",
    "    else:\n",
    "        if a == 1:\n",
    "            L = np.exp(700)\n",
    "        else:\n",
    "            L = -np.log(1-a)\n",
    "    L = float(L)\n",
    "    #########################################\n",
    "    return L    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(x,y,w,b):\n",
    "    '''\n",
    "       Forward pass: given an instance in the training data, compute the logit z, activation a and cross entropy L on the instance. \n",
    "        Input:\n",
    "            x: the feature vector of a training instance, a float numpy matrix of shape p by 1. Here p is the number of features/dimensions.\n",
    "            y: the label of a training instance, an integer scalar value. The values can be 0 or 1.\n",
    "            w: the weight vector, a float numpy matrix of shape p by 1.\n",
    "            b: the bias value, a float scalar.\n",
    "        Output:\n",
    "            z: linear logit of the instance, a float scalar\n",
    "            a: activation, a float scalar\n",
    "            L: the cross entropy loss on the training instance, a float scalar. \n",
    "        Hint: you could solve this problem using 3 lines of code. Though using more lines of code is also okay.\n",
    "    '''\n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "    z = compute_z(x, w, b)\n",
    "    a = compute_a(z)\n",
    "    L = compute_L(a,y)\n",
    "\n",
    "    #########################################\n",
    "    return z, a, L "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "# Compute Local Gradients\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "\n",
    "#--------------------------\n",
    "def compute_dL_da(a, y):\n",
    "    '''\n",
    "        Compute local gradient of the cross-entropy function (the Loss function) L w.r.t. the activation a.\n",
    "        Input:\n",
    "            a: the activation value, a float scalar\n",
    "            y: the label of a training instance, an integer scalar value. The values can be 0 or 1.\n",
    "        Output:\n",
    "            dL_da: the local gradient of the loss function w.r.t. the activation, a float scalar value.\n",
    "    if y == 0 and a !=1:\n",
    "        dL_da = 1/(1 - a)\n",
    "    elif y == 0 and a ==1:\n",
    "        dL_da = 1e5+1\n",
    "    elif y ==1 and a !=0:\n",
    "        dL_da = -1/a\n",
    "    else:\n",
    "        dL_da =1e5+1\n",
    "        \n",
    "    if y==0:\n",
    "        dL_da = 1/(1-a)\n",
    "    else:\n",
    "        dL_da = -1/a\n",
    "    float(1 / (1 + np.exp(700)))\n",
    "    '''\n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "    if y == 0 and a !=1:\n",
    "        dL_da = 1/(1 - a)\n",
    "    elif y == 0 and a ==1:\n",
    "        dL_da = np.exp(700)\n",
    "    elif y ==1 and a !=0:\n",
    "        dL_da = -1/a\n",
    "    else:\n",
    "        dL_da =np.exp(700)\n",
    "        \n",
    "    #########################################\n",
    "    return dL_da \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------------------------\n",
    "def compute_da_dz(a):\n",
    "    '''\n",
    "        Compute local gradient of the sigmoid activation a w.r.t. the logit z.\n",
    "        Input:\n",
    "            a: the activation value of the sigmoid function, a float scalar\n",
    "        Output:\n",
    "            da_dz: the local gradient of the activation w.r.t. the logit z, a float scalar value.\n",
    "        Hint: the gradient da_dz only depends on the activation a, instead of the logit z.\n",
    "        Hint: you could solve this problem using 1 line of code.\n",
    "    '''\n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "    da_dz = a*(1 - a)\n",
    "\n",
    "\n",
    "    #########################################\n",
    "    return da_dz \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#--------------------------\n",
    "def compute_dz_dw(x):\n",
    "    '''\n",
    "        Compute partial gradients of the logit function z with respect to (w.r.t.) the weights w. \n",
    "        Input:\n",
    "            x: the feature vector of a data instance, a float numpy matrix of shape p by 1. \n",
    "               Here p is the number of features/dimensions.\n",
    "        Output:\n",
    "            dz_dw: the partial gradients of the logit z with respect to the weights w, a numpy float matrix of shape p by 1. \n",
    "                   The i-th element represents ( d_z / d_w[i])\n",
    "        Hint: you could solve this problem using 1 line of code. \n",
    "    '''\n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "\n",
    "    dz_dw = x\n",
    "    #########################################\n",
    "    return dz_dw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------------------------\n",
    "def compute_dz_db():\n",
    "    '''\n",
    "        Compute partial gradient of the logit function z with respect to (w.r.t.) the bias b. \n",
    "        Output:\n",
    "            dz_db: the partial gradient of logit z with respect to the bias b, a float scalar. It represents (d_z / d_b).\n",
    "    '''\n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "\n",
    "\n",
    "    dz_db = 1\n",
    "    #########################################\n",
    "    return dz_db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "# Back Propagation \n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "#--------------------------\n",
    "def backward(x,y,a):\n",
    "    '''\n",
    "       Back Propagation: given an instance in the training data, compute the local gradients for logit, activation, weights and bias on the instance. \n",
    "        Input:\n",
    "            x: the feature vector of a data instance, a float numpy matrix of shape p by 1. \n",
    "            y: the label of a training instance, an integer scalar value. The values can be 0 or 1.\n",
    "            a: the activation, a float scalar\n",
    "        Output:\n",
    "            dL_da: the local gradient of the loss function w.r.t. the activation, a float scalar value.\n",
    "            da_dz: the local gradient of the activation a w.r.t. the logit z, a float scalar value. It represents ( d_a / d_z )\n",
    "            dz_dw: the partial gradient of logit z with respect to the weight vector, a numpy float matrix of shape (p by 1). \n",
    "                   The i-th element represents ( d_z / d_w[i])\n",
    "            dz_db: the partial gradient of logit z with respect to the bias, a float scalar. It represents (d_z / d_b).\n",
    "    '''\n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "    dL_da = compute_dL_da(a, y)\n",
    "    da_dz = compute_da_dz(a)\n",
    "    dz_dw = compute_dz_dw(x)\n",
    "    dz_db = compute_dz_db()\n",
    "\n",
    "    #########################################\n",
    "    return dL_da, da_dz, dz_dw, dz_db \n",
    "\n",
    "\n",
    "\n",
    "#--------------------------\n",
    "def compute_dL_dw(dL_da, da_dz, dz_dw):\n",
    "    '''\n",
    "       Given local gradients, compute the gradient of the loss function L w.r.t. the weights w.\n",
    "        Input:\n",
    "            dL_da: the local gradient of the loss function w.r.t. the activation, a float scalar value.\n",
    "            da_dz: the local gradient of the activation a w.r.t. the logit z, a float scalar value. It represents ( d_a / d_z )\n",
    "            dz_dw: the partial gradient of logit z with respect to the weight vector, a numpy float matrix of shape (p by 1). \n",
    "                   The i-th element represents ( d_z / d_w[i])\n",
    "        Output:\n",
    "            dL_dw: the gradient of the loss function w.r.t. the weight vector, a numpy float matrix of shape (p by 1). \n",
    "        Hint: you could solve this problem using 1 lines of code\n",
    "    '''\n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "    dL_dw = dL_da * da_dz * dz_dw\n",
    "\n",
    "    #########################################\n",
    "    return dL_dw\n",
    "\n",
    "\n",
    "#--------------------------\n",
    "def compute_dL_db(dL_da, da_dz, dz_db):\n",
    "    '''\n",
    "       Given the local gradients, compute the gradient of the loss function L w.r.t. bias b.\n",
    "        Input:\n",
    "            dL_da: the local gradient of the loss function w.r.t. the activation, a float scalar value.\n",
    "            da_dz: the local gradient of the activation a w.r.t. the logit z, a float scalar value. It represents ( d_a / d_z )\n",
    "            dz_db: the partial gradient of logit z with respect to the bias, a float scalar. It represents (d_z / d_b).\n",
    "        Output:\n",
    "            dL_db: the gradient of the loss function w.r.t. the bias, a float scalar. \n",
    "        Hint: you could solve this problem using 1 lines of code \n",
    "    '''\n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "\n",
    "    dL_db = dL_da * da_dz * dz_db\n",
    "    #########################################\n",
    "    return dL_db \n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# gradient descent \n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "#--------------------------\n",
    "def update_w(w, dL_dw, alpha=0.001):\n",
    "    '''\n",
    "       Given an instance in the training data, update the weights w using gradient descent.\n",
    "        Input:\n",
    "            w: the current value of the weight vector, a numpy float matrix of shape p by 1.\n",
    "            dL_dw: the gradient of the loss function w.r.t. the weight vector, a numpy float matrix of shape p by 1. \n",
    "            alpha: the step-size parameter of gradient descent, a float scalar.\n",
    "        Output:\n",
    "            w: the updated weight vector, a numpy float matrix of shape p by 1.\n",
    "        Hint: you could solve this problem using 1 line of code\n",
    "    '''\n",
    "    \n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "\n",
    "    w = w - alpha * dL_dw\n",
    "    #########################################\n",
    "    return w\n",
    "\n",
    "#--------------------------\n",
    "def update_b(b, dL_db, alpha=0.001):\n",
    "    '''\n",
    "       Given an instance in the training data, update the bias b using gradient descent.\n",
    "        Input:\n",
    "            b: the current value of bias, a float scalar. \n",
    "            dL_db: the gradient of the loss function w.r.t. the bias, a float scalar. \n",
    "            alpha: the step-size parameter of gradient descent, a float scalar.\n",
    "        Output:\n",
    "            b: the updated of bias, a float scalar. \n",
    "        Hint: you could solve this problem using 1 line of code in the block.\n",
    "    '''\n",
    "    \n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "    b = b - alpha * dL_db\n",
    "\n",
    "    #########################################\n",
    "    return  b \n",
    "\n",
    "\n",
    "#--------------------------\n",
    "def train(X, Y, alpha=0.001, n_epoch=100):\n",
    "    '''\n",
    "       Given a training dataset, train the logistic regression model by iteratively updating the weights w and bias b using the gradients computed over each data instance. \n",
    "We repeat n_epoch passes over all the training instances.\n",
    "        Input:\n",
    "            X: the feature matrix of training instances, a float numpy matrix of shape (n by p). Here n is the number of data instance in the training set, p is the number of features/dimensions.\n",
    "            Y: the labels of training instance, a numpy integer matrix of shape n by 1. The values can be 0 or 1.\n",
    "            alpha: the step-size parameter of gradient descent, a float scalar.\n",
    "            n_epoch: the number of passes to go through the training set, an integer scalar.\n",
    "        Output:\n",
    "            w: the weight vector trained on the training set, a numpy float matrix of shape p by 1.\n",
    "            b: the bias, a float scalar. \n",
    "    '''\n",
    "\n",
    "    # initialize weights and biases as 0\n",
    "    w, b = np.mat(np.zeros(X.shape[1])).T, 0.\n",
    "\n",
    "    for _ in xrange(n_epoch):\n",
    "        for x,y in zip(X,Y):\n",
    "            x = x.T # convert to column vector\n",
    "            #########################################\n",
    "            ## INSERT YOUR CODE HERE\n",
    "\n",
    "            # Forward pass: compute the logit, sigmoid activation and cross_entropy loss function.\n",
    "            z,a,l = forward(x, y, w, b)\n",
    "\n",
    "            # Back propagation: compute local gradients \n",
    "            dL_da, da_dz, dz_dw, dz_db = backward(x,y,a)\n",
    "\n",
    "            # compute the global gradients using chain rule \n",
    "            dL_dw = compute_dL_dw(dL_da, da_dz, dz_dw)\n",
    "            dL_db = compute_dL_db(dL_da, da_dz, dz_db)\n",
    "            # update the parameters w and b\n",
    "            w = update_w(w, dL_dw)\n",
    "            b = update_b(b, dL_db)\n",
    "            #########################################\n",
    "    return w, b\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------\n",
    "def predict(Xtest, w, b):\n",
    "    '''\n",
    "       Predict the labels of the instances in a test dataset using logistic regression.\n",
    "        Input:\n",
    "            Xtest: the feature matrix of testing instances, a float numpy matrix of shape (n_test by p). Here n_test is the number of data instance in the test set, p is the number of features/dimensions.\n",
    "            w: the weight vector of the logistic model, a float numpy matrix of shape p by 1.\n",
    "            b: the bias value of the logistic model, a float scalar.\n",
    "        Output:\n",
    "            Y: the predicted labels of test data, an integer numpy array of length ntest. \n",
    "                    If the predicted label is positive, the value is 1. If the label is negative, the value is 0.\n",
    "            P: the predicted probability of test data to have positive labels, a float numpy matrix of shape ntest by 1. \n",
    "                    Each value is between 0 and 1, indicating the probability of the instance having the positive label. \n",
    "            Note: If the activation is 0.5, we consider the prediction as positive (instead of negative).\n",
    "    '''\n",
    "    n = Xtest.shape[0]\n",
    "    Y = np.zeros(n) # initialize as all zeros\n",
    "    P = np.mat(np.zeros((n,1)))\n",
    "    for i, x in enumerate(Xtest):\n",
    "        x = x.T # convert to column vector\n",
    "        #########################################\n",
    "        ## INSERT YOUR CODE HERE\n",
    "        z = compute_z(x,w,b)\n",
    "        a = compute_a(z)\n",
    "        P[i] = a\n",
    "        if a >= 0.5:\n",
    "            Y[i] = 1\n",
    "        else:\n",
    "            Y[i] = 0\n",
    "        #########################################\n",
    "    return Y, P\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# gradient checking \n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "\n",
    "#--------------------------\n",
    "def check_dL_da(a, y, delta=1e-7):\n",
    "    '''\n",
    "        Compute local gradient of the cross-entropy function w.r.t. the activation using gradient checking.\n",
    "        Input:\n",
    "            a: the activation value, a float scalar\n",
    "            y: the label of a training instance, an integer scalar value. The values can be 0 or 1.\n",
    "            delta: a small number for gradient check, a float scalar.\n",
    "        Output:\n",
    "            dL_da: the approximated local gradient of the loss function w.r.t. the activation, a float scalar value.\n",
    "    '''\n",
    "    dL_da = (compute_L(a+delta,y) - compute_L(a,y)) / delta\n",
    "    return dL_da \n",
    "\n",
    "\n",
    "#--------------------------\n",
    "def check_da_dz(z, delta= 1e-7):\n",
    "    '''\n",
    "        Compute local gradient of the sigmoid function using gradient check.\n",
    "        Input:\n",
    "            z: the logit value of logistic regression, a float scalar.\n",
    "            delta: a small number for gradient check, a float scalar.\n",
    "        Output:\n",
    "            da_dz: the approximated local gradient of activation a w.r.t. the logit z, a float scalar value.\n",
    "    '''\n",
    "    da_dz = (compute_a(z+delta) - compute_a(z)) / delta\n",
    "    return da_dz \n",
    "\n",
    "\n",
    "\n",
    "#--------------------------\n",
    "def check_dz_dw(x,w, b, delta=1e-7):\n",
    "    '''\n",
    "        compute the partial gradients of the logit function z w.r.t. weights w using gradient checking.\n",
    "        The idea is to add a small number to the weights and b separately, and approximate the true gradient using numerical gradient.\n",
    "        Input:\n",
    "            x: the feature vector of a data instance, a float numpy vector of length p. Here p is the number of features/dimensions.\n",
    "            w: the weight vector of the logistic model, a float numpy vector of length p. \n",
    "            b: the bias value of the logistic model, a float scalar.\n",
    "            delta: a small number for gradient check, a float scalar.\n",
    "        Output:\n",
    "            dz_dw: the approximated partial gradient of logit z w.r.t. the weight vector w computed using gradient check, a numpy float vector of length p. \n",
    "    '''\n",
    "    p = x.shape[0] \n",
    "    dz_dw = np.mat(np.zeros(p)).T\n",
    "    for i in xrange(p):\n",
    "        d = np.mat(np.zeros(p)).T\n",
    "        d[i] = delta\n",
    "        dz_dw[i] = (compute_z(x,w+d, b) - compute_z(x, w, b)) / delta\n",
    "    return dz_dw\n",
    "\n",
    "\n",
    "#--------------------------\n",
    "def check_dz_db(x,w, b, delta=1e-7):\n",
    "    '''\n",
    "        compute the partial gradients of the logit function z w.r.t. the bias b using gradient checking.\n",
    "        The idea is to add a small number to the weights and b separately, and approximate the true gradient using numerical gradient.\n",
    "        For example, the true gradient of logit z w.r.t. bias can be approximated as  [z(w,b+ delta) - z(w,b)] / delta , here delta is a small number.\n",
    "        Input:\n",
    "            x: the feature vector of a data instance, a float numpy vector of length p. Here p is the number of features/dimensions.\n",
    "            w: the weight vector of the logistic model, a float numpy vector of length p. \n",
    "            b: the bias value of the logistic model, a float scalar.\n",
    "            delta: a small number for gradient check, a float scalar.\n",
    "        Output:\n",
    "            dz_dw: the approximated partial gradient of logit z w.r.t. the weight vector w computed using gradient check, a numpy float vector of length p. \n",
    "            dz_db: the approximated partial gradient of logit z w.r.t. the bias b using gradient check, a float scalar.\n",
    "    '''\n",
    "    dz_db = (compute_z(x, w, b+delta) - compute_z(x, w, b)) / delta\n",
    "    return  dz_db\n",
    "\n",
    "#--------------------------\n",
    "def check_dL_dw(x,y,w,b, delta=1e-7):\n",
    "    '''\n",
    "       Given an instance in the training data, compute the gradient of the weights w using gradient check.\n",
    "        Input:\n",
    "            x: the feature vector of a training instance, a float numpy vector of length p. Here p is the number of features/dimensions.\n",
    "            y: the label of a training instance, an integer scalar value. The values can be 0 or 1.\n",
    "            w: the weight vector, a float numpy vector of length p.\n",
    "            b: the bias value, a float scalar.\n",
    "            delta: a small number for gradient check, a float scalar.\n",
    "        Output:\n",
    "            dL_dw: the approximated gradient of the loss function w.r.t. the weight vector, a numpy float vector of length p. \n",
    "    '''\n",
    "    p = x.shape[0] # number of features\n",
    "    dL_dw = np.mat(np.zeros(p)).T\n",
    "    for i in xrange(p):\n",
    "        d = np.mat(np.zeros(p)).T\n",
    "        d[i] = delta\n",
    "        dL_dw[i] = (forward(x,y,w+d,b)[-1] - forward(x,y,w,b)[-1]) / delta\n",
    "    return dL_dw\n",
    "\n",
    "#--------------------------\n",
    "def check_dL_db(x,y,w,b, delta=1e-7):\n",
    "    '''\n",
    "       Given an instance in the training data, compute the gradient of the bias b using gradient check.\n",
    "        Input:\n",
    "            x: the feature vector of a training instance, a float numpy vector of length p. Here p is the number of features/dimensions.\n",
    "            y: the label of a training instance, an integer scalar value. The values can be 0 or 1.\n",
    "            w: the weight vector, a float numpy vector of length p.\n",
    "            b: the bias value, a float scalar.\n",
    "            delta: a small number for gradient check, a float scalar.\n",
    "        Output:\n",
    "            dL_db: the approximated gradient of the loss function w.r.t. the bias, a float scalar. \n",
    "    '''\n",
    "    dL_db = (forward(x,y,w,b+delta)[-1] - forward(x,y,w,b)[-1]) / delta\n",
    "    return dL_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.09763782]\n",
      " [-0.00114751]] -0.00231425885429\n",
      "[[-1.11825490e-05]\n",
      " [ 9.97882078e-03]] -2.23859390033e-05\n",
      "[[0.58923992]\n",
      " [0.58906044]] -0.411212961101\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-6ccbd6a4a1a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32massert\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32massert\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32massert\u001b[0m  \u001b[0mb\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
