{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "'''\n",
    "    Problem 3: softmax regression \n",
    "    In this problem, you will implement the softmax regression for multi-class classification problems.\n",
    "    The main goal of this problem is to extend the logistic regression method to solving multi-class classification problems.\n",
    "    We will get familiar with computing gradients of vectors/matrices.\n",
    "    We will use multi-class cross entropy as the loss function and stochastic gradient descent to train the model parameters.\n",
    "    You could test the correctness of your code by typing `nosetests test3.py` in the terminal.\n",
    "\n",
    "    Notations:\n",
    "            ---------- input data ----------------------\n",
    "            p: the number of input features, an integer scalar.\n",
    "            c: the number of classes in the classification task, an integer scalar.\n",
    "            x: the feature vector of a data instance, a float numpy matrix of shape p by 1. \n",
    "            y: the label of a training instance, an integer scalar value. The values can be 0,1,2, ..., or (c-1).\n",
    "\n",
    "            ---------- model parameters ----------------------\n",
    "            W: the weight matrix of softmax regression, a float numpy matrix of shape (c by p). \n",
    "            b: the bias values of softmax regression, a float numpy matrix of shape c by 1.\n",
    "            ---------- values ----------------------\n",
    "            z: the linear logits, a float numpy matrix of shape c by 1.\n",
    "            a: the softmax activations, a float numpy matrix of shape c by 1. \n",
    "            L: the multi-class cross entropy loss, a float scalar.\n",
    "\n",
    "            ---------- partial gradients ----------------------\n",
    "            dL_da: the partial gradients of the loss function L w.r.t. the activations a, a float numpy matrix of shape c by 1. \n",
    "                   The i-th element dL_da[i] represents the partial gradient of the loss function L w.r.t. the i-th activation a[i]:  d_L / d_a[i].\n",
    "            da_dz: the partial gradient of the activations a w.r.t. the logits z, a float numpy matrix of shape (c by c). \n",
    "                   The (i,j)-th element of da_dz represents the partial gradient ( d_a[i]  / d_z[j] )\n",
    "            dz_dW: the partial gradient of logits z w.r.t. the weight matrix W, a numpy float matrix of shape (c by p). \n",
    "                   The (i,j)-th element of dz_dW represents the partial gradient of the i-th logit (z[i]) w.r.t. the weight W[i,j]:   d_z[i] / d_W[i,j]\n",
    "            dz_db: the partial gradient of the logits z w.r.t. the biases b, a float matrix of shape c by 1. \n",
    "                   Each element dz_db[i] represents the partial gradient of the i-th logit z[i] w.r.t. the i-th bias b[i]:  d_z[i] / d_b[i]\n",
    "\n",
    "            ---------- partial gradients of parameters ------------------\n",
    "            dL_dW: the partial gradients of the loss function L w.r.t. the weight matrix W, a numpy float matrix of shape (c by p). \n",
    "                   The i,j-th element dL_dW[i,j] represents the partial gradient of the loss function L w.r.t. the i,j-th weight W[i,j]:  d_L / d_W[i,j]\n",
    "            dL_db: the partial gradient of the loss function L w.r.t. the biases b, a float numpy matrix of shape c by 1.\n",
    "                   The i-th element dL_db[i] represents the partial gradient of the loss function w.r.t. the i-th bias:  d_L / d_b[i]\n",
    "\n",
    "            ---------- training ----------------------\n",
    "            alpha: the step-size parameter of gradient descent, a float scalar.\n",
    "            n_epoch: the number of passes to go through the training dataset in order to train the model, an integer scalar.\n",
    "'''\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# Forward Pass \n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "def compute_z(x,W,b):\n",
    "    '''\n",
    "        Compute the linear logit values of a data instance. z =  W x + b\n",
    "        Input:\n",
    "            x: the feature vector of a data instance, a float numpy matrix of shape p by 1. Here p is the number of features/dimensions.\n",
    "            W: the weight matrix of softmax regression, a float numpy matrix of shape (c by p). Here c is the number of classes.\n",
    "            b: the bias values of softmax regression, a float numpy vector of shape c by 1.\n",
    "        Output:\n",
    "            z: the linear logits, a float numpy vector of shape c by 1. \n",
    "        Hint: you could solve this problem using 1 line of code.\n",
    "    '''\n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "\n",
    "    z = W*x + b\n",
    "\n",
    "    #########################################\n",
    "    return z \n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "def compute_a(z):\n",
    "    '''\n",
    "        Compute the softmax activations.\n",
    "        Input:\n",
    "            z: the logit values of softmax regression, a float numpy vector of shape c by 1. Here c is the number of classes\n",
    "        Output:\n",
    "            a: the softmax activations, a float numpy vector of shape c by 1. \n",
    "    '''\n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "    a =[]\n",
    "    sumz=0\n",
    "    ez=[]\n",
    "    for i in range(z.shape[0]):\n",
    "        if z[i,0]>700 :\n",
    "            dang=np.exp(700)\n",
    "            ez.append(dang)\n",
    "        elif z[i,0]<-700:\n",
    "            dang=np.exp(-700)\n",
    "            ez.append(dang)\n",
    "        else:\n",
    "            dang=np.exp(z[i,0])\n",
    "            ez.append(dang)\n",
    "        sumz=sumz+dang\n",
    "    for e in ez:\n",
    "        a.append(float(e/sumz))\n",
    "    a=np.asmatrix(a).reshape((z.shape[0],1))    \n",
    "    #########################################\n",
    "    return a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "def compute_L(a,y):\n",
    "    '''\n",
    "        Compute multi-class cross entropy, which is the loss function of softmax regression. \n",
    "        Input:\n",
    "            a: the activations of a training instance, a float numpy vector of shape c by 1. Here c is the number of classes. \n",
    "            y: the label of a training instance, an integer scalar value. The values can be 0,1,2, ..., or (c-1).\n",
    "        Output:\n",
    "            L: the loss value of softmax regression, a float scalar.\n",
    "    '''\n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "    a=np.asarray(a)\n",
    "    if a[y]==0:\n",
    "        tmp=-np.inf\n",
    "    else:\n",
    "        tmp=np.log(a[y])\n",
    "    L=float(-tmp)\n",
    "\n",
    "\n",
    "    #########################################\n",
    "    return L \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "def forward(x,y,W,b):\n",
    "    '''\n",
    "       Forward pass: given an instance in the training data, compute the logits z, activations a and multi-class cross entropy L on the instance.\n",
    "        Input:\n",
    "            x: the feature vector of a training instance, a float numpy vector of shape p by 1. Here p is the number of features/dimensions.\n",
    "            y: the label of a training instance, an integer scalar value. The values can be 0 or 1.\n",
    "            W: the weight matrix of softmax regression, a float numpy matrix of shape (c by p). Here c is the number of classes.\n",
    "            b: the bias values of softmax regression, a float numpy vector of shape c by 1.\n",
    "        Output:\n",
    "            z: the logit values of softmax regression, a float numpy vector of shape c by 1. Here c is the number of classes\n",
    "            a: the activations of a training instance, a float numpy vector of shape c by 1. Here c is the number of classes. \n",
    "            L: the loss value of softmax regression, a float scalar.\n",
    "    '''\n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "    z = compute_z(x,W,b)\n",
    "    a = compute_a(z)\n",
    "    L = compute_L(a,y) \n",
    "\n",
    "\n",
    "    #########################################\n",
    "    return z, a, L "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "# Compute Local Gradients\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "def compute_dL_da(a, y):\n",
    "    '''\n",
    "        Compute local gradient of the multi-class cross-entropy loss function w.r.t. the activations.\n",
    "        Input:\n",
    "            a: the activations of a training instance, a float numpy vector of shape c by 1. Here c is the number of classes. \n",
    "            y: the label of a training instance, an integer scalar value. The values can be 0,1,2, ..., or (c-1).\n",
    "        Output:\n",
    "            dL_da: the local gradients of the loss function w.r.t. the activations, a float numpy vector of shape c by 1. \n",
    "                   The i-th element dL_da[i] represents the partial gradient of the loss function w.r.t. the i-th activation a[i]:  d_L / d_a[i].\n",
    "    '''\n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE \n",
    "\n",
    "#     if a[y] ==0:\n",
    "#         a[y] = np.exp(-700)\n",
    "#     dL_da=[]\n",
    "#     for i in range(a.shape[0]):\n",
    "#         if i == y:\n",
    "#             dL_da.append(float(-1/a[y]))\n",
    "#         else:\n",
    "#             dL_da.append(float(0.0))\n",
    "#     dL_da=np.asmatrix(dL_da).reshape(a.shape[0],a.shape[1])\n",
    "    print \"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\"\n",
    "    if a[y] ==0:\n",
    "        a[y] = np.exp(-700)\n",
    "    dL_da=a.shape[0.0]*[0]\n",
    "    dL_da[y] = -1 / a[y]\n",
    "    dL_da=np.asmatrix(dL_da).reshape(a.shape[0],a.shape[1])\n",
    "\n",
    "    #########################################\n",
    "    return dL_da \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "def compute_da_dz(a):\n",
    "    '''\n",
    "        Compute local gradient of the softmax activations a w.r.t. the logits z.\n",
    "        Input:\n",
    "            a: the activation values of softmax function, a numpy float vector of shape c by 1. Here c is the number of classes.\n",
    "        Output:\n",
    "            da_dz: the local gradient of the activations a w.r.t. the logits z, a float numpy matrix of shape (c by c). \n",
    "                   The (i,j)-th element of da_dz represents the partial gradient ( d_a[i]  / d_z[j] )\n",
    "        Hint: you could solve this problem using 4 or 5 lines of code.\n",
    "        (3 points)\n",
    "    '''\n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "    c = a.shape[0]\n",
    "    a = np.array(a)    \n",
    "    d = np.mat(np.zeros(c**2)).reshape((c,c))\n",
    "    for i in range(c):\n",
    "        for j in range(c):\n",
    "            if i == j:\n",
    "                d[i,j]=a[i]*(1-a[i])\n",
    "            else:\n",
    "                d[i,j]=-a[i]*a[j]\n",
    "\n",
    "    da_dz = d\n",
    "    #########################################\n",
    "    return da_dz \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "def compute_dz_dW(x,c):\n",
    "    '''\n",
    "        Compute local gradient of the logits function z w.r.t. the weights W.\n",
    "        Input:\n",
    "            x: the feature vector of a data instance, a float numpy vector of shape p by 1. Here p is the number of features/dimensions.\n",
    "            c: the number of classes, an integer. \n",
    "        Output:\n",
    "            dz_dW: the partial gradient of logits z w.r.t. the weight matrix, a numpy float matrix of shape (c by p). \n",
    "                   The (i,j)-th element of dz_dW represents the partial gradient of the i-th logit (z[i]) w.r.t. the weight W[i,j]:   d_z[i] / d_W[i,j]\n",
    "        Hint: the partial gradients only depend on the input x and the number of classes \n",
    "    '''\n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "    dz_dW = np.repeat(x,c,axis=1).T\n",
    "\n",
    "    #########################################\n",
    "    return dz_dW\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "def compute_dz_db(c):\n",
    "    '''\n",
    "        Compute local gradient of the logits function z w.r.t. the biases b. \n",
    "        Input:\n",
    "            c: the number of classes, an integer. \n",
    "        Output:\n",
    "            dz_db: the partial gradient of the logits z w.r.t. the biases b, a float vector of shape c by 1. \n",
    "                   Each element dz_db[i] represents the partial gradient of the i-th logit z[i] w.r.t. the i-th bias b[i]:  d_z[i] / d_b[i]\n",
    "        Hint: you could solve this problem using 1 line of code.\n",
    "    '''\n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "    dz_db = np.asmatrix(np.repeat(1,c)).T\n",
    "\n",
    "    #########################################\n",
    "    return dz_db\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "# Back Propagation \n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "def backward(x,y,a):\n",
    "    '''\n",
    "       Back Propagation: given an instance in the training data, compute the local gradients of the logits z, activations a, weights W and biases b on the instance. \n",
    "        Input:\n",
    "            x: the feature vector of a training instance, a float numpy vector of shape p by 1. Here p is the number of features/dimensions.\n",
    "            y: the label of a training instance, an integer scalar value. The values can be 0,1,2, ..., or (c-1).\n",
    "            a: the activations of a training instance, a float numpy vector of shape c by 1. Here c is the number of classes. \n",
    "        Output:\n",
    "            dL_da: the local gradients of the loss function w.r.t. the activations, a float numpy vector of shape c by 1. \n",
    "                   The i-th element dL_da[i] represents the partial gradient of the loss function L w.r.t. the i-th activation a[i]:  d_L / d_a[i].\n",
    "            da_dz: the local gradient of the activation w.r.t. the logits z, a float numpy matrix of shape (c by c). \n",
    "                   The (i,j)-th element of da_dz represents the partial gradient ( d_a[i]  / d_z[j] )\n",
    "            dz_dW: the partial gradient of logits z w.r.t. the weight matrix W, a numpy float matrix of shape (c by p). \n",
    "                   The i,j -th element of dz_dW represents the partial gradient of the i-th logit (z[i]) w.r.t. the weight W[i,j]:   d_z[i] / d_W[i,j]\n",
    "            dz_db: the partial gradient of the logits z w.r.t. the biases b, a float vector of shape c by 1. \n",
    "                   Each element dz_db[i] represents the partial gradient of the i-th logit z[i] w.r.t. the i-th bias:  d_z[i] / d_b[i]\n",
    "    '''\n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "    dL_da=compute_dL_da(a, y)\n",
    "    da_dz=compute_da_dz(a)\n",
    "    dz_dW=compute_dz_dW(x,a.shape[0])\n",
    "    dz_db=compute_dz_db(a.shape[0])\n",
    "\n",
    "\n",
    "    #########################################\n",
    "    return dL_da, da_dz, dz_dW, dz_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "def compute_dL_dz(dL_da,da_dz):\n",
    "    '''\n",
    "       Given the local gradients, compute the gradient of the loss function L w.r.t. the logits z using chain rule.\n",
    "        Input:\n",
    "            dL_da: the local gradients of the loss function w.r.t. the activations, a float numpy vector of shape c by 1. \n",
    "                   The i-th element dL_da[i] represents the partial gradient of the loss function L w.r.t. the i-th activation a[i]:  d_L / d_a[i].\n",
    "            da_dz: the local gradient of the activation w.r.t. the logits z, a float numpy matrix of shape (c by c). \n",
    "                   The (i,j)-th element of da_dz represents the partial gradient ( d_a[i]  / d_z[j] )\n",
    "        Output:\n",
    "            dL_dz: the gradient of the loss function L w.r.t. the logits z, a numpy float vector of shape c by 1. \n",
    "                   The i-th element dL_dz[i] represents the partial gradient of the loss function L w.r.t. the i-th logit z[i]:  d_L / d_z[i].\n",
    "    '''\n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "    dL_dz = da_dz*dL_da\n",
    "\n",
    "\n",
    "    #########################################\n",
    "    return dL_dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "def compute_dL_dW(dL_dz,dz_dW):\n",
    "    '''\n",
    "       Given the local gradients, compute the gradient of the loss function L w.r.t. the weights W using chain rule. \n",
    "        Input:\n",
    "            dL_dz: the gradient of the loss function L w.r.t. the logits z, a numpy float vector of shape c by 1. \n",
    "                   The i-th element dL_dz[i] represents the partial gradient of the loss function L w.r.t. the i-th logit z[i]:  d_L / d_z[i].\n",
    "            dz_dW: the partial gradient of logits z w.r.t. the weight matrix W, a numpy float matrix of shape (c by p). \n",
    "                   The i,j -th element of dz_dW represents the partial gradient of the i-th logit (z[i]) w.r.t. the weight W[i,j]:   d_z[i] / d_W[i,j]\n",
    "        Output:\n",
    "            dL_dW: the global gradient of the loss function w.r.t. the weight matrix, a numpy float matrix of shape (c by p). \n",
    "                   Here c is the number of classes.\n",
    "                   The i,j-th element dL_dW[i,j] represents the partial gradient of the loss function L w.r.t. the i,j-th weight W[i,j]:  d_L / d_W[i,j]\n",
    "        Hint: you could solve this problem using 2 lines of code\n",
    "    '''\n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "    dL_dW = np.array(dL_dz)*np.array(dz_dW)\n",
    "    dL_dW=np.asmatrix(dL_dW)\n",
    "    #########################################\n",
    "    return dL_dW\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "def compute_dL_db(dL_dz,dz_db):\n",
    "    '''\n",
    "       Given the local gradients, compute the gradient of the loss function L w.r.t. the biases b using chain rule.\n",
    "        Input:\n",
    "            dL_dz: the gradient of the loss function L w.r.t. the logits z, a numpy float vector of shape c by 1. \n",
    "                   The i-th element dL_dz[i] represents the partial gradient of the loss function L w.r.t. the i-th logit z[i]:  d_L / d_z[i].\n",
    "            dz_db: the local gradient of the logits z w.r.t. the biases b, a float numpy vector of shape c by 1. \n",
    "                   The i-th element dz_db[i] represents the partial gradient ( d_z[i]  / d_b[i] )\n",
    "        Output:\n",
    "            dL_db: the global gradient of the loss function L w.r.t. the biases b, a float numpy vector of shape c by 1.\n",
    "                   The i-th element dL_db[i] represents the partial gradient of the loss function w.r.t. the i-th bias:  d_L / d_b[i]\n",
    "        Hint: you could solve this problem using 1 line of code in the block.\n",
    "    '''\n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "    dL_db = np.matrix(np.array(dL_dz) * np.array(dz_db))\n",
    "    #########################################\n",
    "    return dL_db \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "# gradient descent \n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "#--------------------------\n",
    "def update_W(W, dL_dW, alpha=0.001):\n",
    "    '''\n",
    "       Update the weights W using gradient descent.\n",
    "        Input:\n",
    "            W: the current weight matrix, a float numpy matrix of shape (c by p). Here c is the number of classes.\n",
    "            alpha: the step-size parameter of gradient descent, a float scalar.\n",
    "            dL_dW: the global gradient of the loss function w.r.t. the weight matrix, a numpy float matrix of shape (c by p). \n",
    "                   The i,j-th element dL_dW[i,j] represents the partial gradient of the loss function L w.r.t. the i,j-th weight W[i,j]:  d_L / d_W[i,j]\n",
    "        Output:\n",
    "            W: the updated weight matrix, a numpy float matrix of shape (c by p).\n",
    "        Hint: you could solve this problem using 1 line of code \n",
    "    '''\n",
    "    \n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "    W = W-alpha*dL_dW\n",
    "    #########################################\n",
    "    return W\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------\n",
    "def update_b(b, dL_db, alpha=0.001):\n",
    "    '''\n",
    "       Update the biases b using gradient descent.\n",
    "        Input:\n",
    "            b: the current bias values, a float numpy vector of shape c by 1.\n",
    "            dL_db: the global gradient of the loss function L w.r.t. the biases b, a float numpy vector of shape c by 1.\n",
    "                   The i-th element dL_db[i] represents the partial gradient of the loss function w.r.t. the i-th bias:  d_L / d_b[i]\n",
    "            alpha: the step-size parameter of gradient descent, a float scalar.\n",
    "        Output:\n",
    "            b: the updated of bias vector, a float numpy vector of shape c by 1. \n",
    "        Hint: you could solve this problem using 1 lines of code \n",
    "    '''\n",
    "    \n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "    b = b-alpha*dL_db\n",
    "\n",
    "    #########################################\n",
    "    return b \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#--------------------------\n",
    "# train\n",
    "def train(X, Y, alpha=0.01, n_epoch=100):\n",
    "    '''\n",
    "       Given a training dataset, train the softmax regression model by iteratively updating the weights W and biases b using the gradients computed over each data instance. \n",
    "        Input:\n",
    "            X: the feature matrix of training instances, a float numpy matrix of shape (n by p). Here n is the number of data instance in the training set, p is the number of features/dimensions.\n",
    "            Y: the labels of training instance, a numpy integer numpy array of length n. The values can be 0 or 1.\n",
    "            alpha: the step-size parameter of gradient ascent, a float scalar.\n",
    "            n_epoch: the number of passes to go through the training set, an integer scalar.\n",
    "        Output:\n",
    "            W: the weight matrix trained on the training set, a numpy float matrix of shape (c by p).\n",
    "            b: the bias, a float numpy vector of shape c by 1. \n",
    "    '''\n",
    "    # number of features\n",
    "    p = X.shape[1]\n",
    "    # number of classes \n",
    "    c = max(Y) + 1\n",
    "\n",
    "    # initialize W and b as 0\n",
    "    W = np.asmatrix(np.zeros((c,p)))\n",
    "    b= np.asmatrix(np.zeros((c,1)))\n",
    "\n",
    "    for _ in xrange(n_epoch):\n",
    "        # go through each training instance\n",
    "        for x,y in zip(X,Y):\n",
    "            x = x.T # convert to column vector\n",
    "            #########################################\n",
    "            ## INSERT YOUR CODE HERE\n",
    "\n",
    "            # Forward pass: compute the logits, softmax and cross_entropy \n",
    "            z,a,l = forward(x, y, W, b)\n",
    "\n",
    "\n",
    "            \n",
    "            # Back Propagation: compute local gradients of cross_entropy, softmax and logits\n",
    "            dL_da, da_dz, dz_dW, dz_db = backward(x,y,a)\n",
    "            dL_dz = compute_dL_dz(dL_da,da_dz)\n",
    "\n",
    "\n",
    "            # compute the global gradients using chain rule \n",
    "            dL_dW = compute_dL_dW(dL_dz,dz_dW)\n",
    "            dL_db = compute_dL_db(dL_dz,dz_db)\n",
    "\n",
    "\n",
    "            # update the paramters using gradient descent\n",
    "            W = update_W(W, dL_dW, alpha)\n",
    "            b = update_b(b, dL_db, alpha)\n",
    "\n",
    "\n",
    "            #########################################\n",
    "    return W, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#--------------------------\n",
    "def predict(Xtest, W, b):\n",
    "    '''\n",
    "       Predict the labels of the instances in a test dataset using softmax regression.\n",
    "        Input:\n",
    "            Xtest: the feature matrix of testing instances, a float numpy matrix of shape (n_test by p). Here n_test is the number of data instance in the test set, p is the number of features/dimensions.\n",
    "            W: the weight vector of the logistic model, a float numpy matrix of shape (c by p).\n",
    "            b: the bias values of the softmax regression model, a float vector of shape c by 1.\n",
    "        Output:\n",
    "            Y: the predicted labels of test data, an integer numpy array of length ntest Each element can be 0, 1, ..., or (c-1) \n",
    "            P: the predicted probabilities of test data to be in different classes, a float numpy matrix of shape (ntest,c). Each (i,j) element is between 0 and 1, indicating the probability of the i-th instance having the j-th class label. \n",
    "        (2 points)\n",
    "    '''\n",
    "    n = Xtest.shape[0]\n",
    "    c = W.shape[0]\n",
    "    Y = np.zeros(n) # initialize as all zeros\n",
    "    P = np.asmatrix(np.zeros((n,c)))  \n",
    "    for i, x in enumerate(Xtest):\n",
    "        x = x.T # convert to column vector\n",
    "        #########################################\n",
    "        ## INSERT YOUR CODE HERE\n",
    "        z=compute_z(x,W,b)\n",
    "        a=compute_a(z)\n",
    "        P[i,]=a.reshape((1,c))\n",
    "        p=P[i,].tolist()[0]\n",
    "        Y[i]=np.array(p.index(max(p)))\n",
    "        #########################################\n",
    "    return Y, P \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "# gradient checking \n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "def check_da_dz(z, delta=1e-7):\n",
    "    '''\n",
    "        Compute local gradient of the softmax function using gradient checking.\n",
    "        Input:\n",
    "            z: the logit values of softmax regression, a float numpy vector of shape c by 1. Here c is the number of classes\n",
    "            delta: a small number for gradient check, a float scalar.\n",
    "        Output:\n",
    "            da_dz: the approximated local gradient of the activations w.r.t. the logits, a float numpy matrix of shape (c by c). \n",
    "                   The (i,j)-th element represents the partial gradient ( d a[i]  / d z[j] )\n",
    "    '''\n",
    "    c = z.shape[0] # number of classes\n",
    "    da_dz = np.asmatrix(np.zeros((c,c)))\n",
    "    for i in xrange(c):\n",
    "        for j in xrange(c):\n",
    "            d = np.asmatrix(np.zeros((c,1)))\n",
    "            d[j] = delta\n",
    "            da_dz[i,j] = (compute_a(z+d)[i,0] - compute_a(z)[i,0]) / delta\n",
    "    return da_dz \n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "def check_dL_da(a, y, delta=1e-7):\n",
    "    '''\n",
    "        Compute local gradient of the multi-class cross-entropy function w.r.t. the activations using gradient checking.\n",
    "        Input:\n",
    "            a: the activations of a training instance, a float numpy vector of shape c by 1. Here c is the number of classes. \n",
    "            y: the label of a training instance, an integer scalar value. The values can be 0,1,2, ..., or (c-1).\n",
    "            delta: a small number for gradient check, a float scalar.\n",
    "        Output:\n",
    "            dL_da: the approximated local gradients of the loss function w.r.t. the activations, a float numpy vector of shape c by 1.\n",
    "    '''\n",
    "    c = a.shape[0] # number of classes\n",
    "    dL_da = np.asmatrix(np.zeros((c,1))) # initialize the vector as all zeros\n",
    "    for i in xrange(c):\n",
    "        d = np.asmatrix(np.zeros((c,1)))\n",
    "        d[i] = delta\n",
    "        dL_da[i] = ( compute_L(a+d,y) \n",
    "                        - compute_L(a,y)) / delta\n",
    "    return dL_da \n",
    "\n",
    "#--------------------------\n",
    "def check_dz_dW(x, W, b, delta=1e-7):\n",
    "    '''\n",
    "        compute the local gradient of the logit function using gradient check.\n",
    "        Input:\n",
    "            x: the feature vector of a data instance, a float numpy vector of shape p by 1. Here p is the number of features/dimensions.\n",
    "            W: the weight matrix of softmax regression, a float numpy matrix of shape (c by p). Here c is the number of classes.\n",
    "            b: the bias values of softmax regression, a float numpy vector of shape c by 1.\n",
    "            delta: a small number for gradient check, a float scalar.\n",
    "        Output:\n",
    "            dz_dW: the approximated local gradient of the logits w.r.t. the weight matrix computed by gradient checking, a numpy float matrix of shape (c by p). \n",
    "                   The i,j -th element of dz_dW represents the partial gradient of the i-th logit (z[i]) w.r.t. the weight W[i,j]:   d_z[i] / d_W[i,j]\n",
    "    '''\n",
    "    c,p = W.shape # number of classes and features\n",
    "    dz_dW = np.asmatrix(np.zeros((c,p)))\n",
    "    for i in xrange(c):\n",
    "        for j in xrange(p):\n",
    "            d = np.asmatrix(np.zeros((c,p)))\n",
    "            d[i,j] = delta\n",
    "            dz_dW[i,j] = (compute_z(x,W+d, b)[i,0] - compute_z(x, W, b))[i,0] / delta\n",
    "    return dz_dW\n",
    "\n",
    "\n",
    "#--------------------------\n",
    "def check_dz_db(x, W, b, delta=1e-7):\n",
    "    '''\n",
    "        compute the local gradient of the logit function using gradient check.\n",
    "        Input:\n",
    "            x: the feature vector of a data instance, a float numpy vector of shape p by 1. Here p is the number of features/dimensions.\n",
    "            W: the weight matrix of softmax regression, a float numpy matrix of shape (c by p). Here c is the number of classes.\n",
    "            b: the bias values of softmax regression, a float numpy vector of shape c by 1.\n",
    "            delta: a small number for gradient check, a float scalar.\n",
    "        Output:\n",
    "            dz_db: the approximated local gradient of the logits w.r.t. the biases using gradient check, a float vector of shape c by 1.\n",
    "                   Each element dz_db[i] represents the partial gradient of the i-th logit z[i] w.r.t. the i-th bias:  d_z[i] / d_b[i]\n",
    "    '''\n",
    "    c,p = W.shape # number of classes and features\n",
    "    dz_db = np.asmatrix(np.zeros((c,1)))\n",
    "    for i in xrange(c):\n",
    "        d = np.asmatrix(np.zeros((c,1))) \n",
    "        d[i] = delta\n",
    "        dz_db[i] = (compute_z(x,W, b+d)[i,0] - compute_z(x, W, b)[i,0]) / delta\n",
    "    return dz_db\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "def check_dL_dW(x,y,W,b,delta=1e-7):\n",
    "    '''\n",
    "       Compute the gradient of the loss function w.r.t. the weights W using gradient checking.\n",
    "        Input:\n",
    "            x: the feature vector of a training instance, a float numpy vector of shape p by 1. Here p is the number of features/dimensions.\n",
    "            y: the label of a training instance, an integer scalar value. The values can be 0,1,2, ..., or (c-1).\n",
    "            W: the weight matrix of softmax regression, a float numpy matrix of shape (c by p). Here c is the number of classes.\n",
    "            b: the bias values of softmax regression, a float numpy vector of shape c by 1.\n",
    "            delta: a small number for gradient check, a float scalar.\n",
    "        Output:\n",
    "            dL_dW: the approximated gradients of the loss function w.r.t. the weight matrix, a numpy float matrix of shape (c by p). \n",
    "    '''\n",
    "    c, p = W.shape    \n",
    "    dL_dW = np.asmatrix(np.zeros((c,p)))\n",
    "    for i in xrange(c):\n",
    "        for j in xrange(p):\n",
    "            d = np.asmatrix(np.zeros((c,p)))\n",
    "            d[i,j] = delta\n",
    "            dL_dW[i,j] = ( forward(x,y,W+d,b)[-1] - forward(x,y,W,b)[-1] ) / delta\n",
    "    return dL_dW\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "def check_dL_db(x,y,W,b,delta=1e-7):\n",
    "    '''\n",
    "       Compute the gradient of the loss function w.r.t. the bias b using gradient checking.\n",
    "        Input:\n",
    "            x: the feature vector of a training instance, a float numpy vector of shape p by 1. Here p is the number of features/dimensions.\n",
    "            y: the label of a training instance, an integer scalar value. The values can be 0,1,2, ..., or (c-1).\n",
    "            W: the weight matrix of softmax regression, a float numpy matrix of shape (c by p). Here c is the number of classes.\n",
    "            b: the bias values of softmax regression, a float numpy vector of shape c by 1.\n",
    "            delta: a small number for gradient check, a float scalar.\n",
    "        Output:\n",
    "            dL_db: the approxmiated gradients of the loss function w.r.t. the biases, a float vector of shape c by 1.\n",
    "    '''\n",
    "    c, p = W.shape    \n",
    "    dL_db = np.asmatrix(np.zeros((c,1)))\n",
    "    for i in xrange(c):\n",
    "        d = np.asmatrix(np.zeros((c,1)))\n",
    "        d[i] = delta\n",
    "        dL_db[i] = ( forward(x,y,W,b+d)[-1] - forward(x,y,W,b)[-1] ) / delta\n",
    "    return dL_db \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
